---
title: '{tidytof} performance benchmarking'
output:
  pdf_document: default
  officedown::rdocx_document:
    mapstyles:
      Normal: First Paragraph
    reference_docx: reference.docx
  html_document:
    df_print: paged
---

```{r setup_bench, include=FALSE}
# options
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE, 
  warning = FALSE,
  fig.cap = TRUE, 
  out.width = "100%"
  )

# libraries
library(tidytof)
library(tidyverse)
library(microbenchmark)
library(flowCore)
filter <- dplyr::filter
count <- dplyr::count

# source benchmarking functions 
source(here::here("manuscript", "benchmarking", "benchmarking_functions.R"))

# globals
machine <- "galaxia"
num_repeats <- 10L
charizard <- c("base" = "#207394", "tidytof" = "#cd5241", "flowcore" = "#eeb45a")

# which benchmarking sections to run from scratch
run_io <- FALSE
run_downsample <- FALSE
run_preprocess <- FALSE
run_tsne <- FALSE
run_pca <- FALSE
run_umap <- FALSE
run_cluster <- FALSE
run_extract <- FALSE
run_style <- FALSE

# file paths 
if (machine == "galaxia") { 
  base_path <- file.path("~", "Box", "Tim", "Lab", "Data")
} else { 
  base_path <- file.path("~", "Desktop", "data")
}
ddpr_path <-
  file.path(base_path, "DDPR_Data")

csv_path <- 
  file.path(base_path, "DDPR_Data_csv")

mini_path <- 
  file.path(base_path, "DDPR_Data_mini")

medium_path <- 
  file.path(base_path, "DDPR_Data_medium")

ddpr_files <- 
  dir(ddpr_path, full.names = TRUE)

single_file <- ddpr_files[[1]]

double_file <- ddpr_files[1:2]
```

```{r read_all_ddpr_data, eval = FALSE}
ddpr_data <- 
  tof_read_data(ddpr_path)
```

```{r gather_ddpr_data_info, eval = FALSE}
ddpr_data_info <- 
  ddpr_data %>% 
  transmute(
    panel,
    num_files = purrr::map_int(.x = data, ~n_distinct(.x$file_name)), 
    file_names = purrr::map(.x = data, ~unique(.x$file_name))
  )

ddpr_data_info %>% 
  write_rds(
    file = here::here("manuscript", "benchmarking", "ddpr_data_info.rds")
  )
```

```{r read_in_ddpr_data_info}
ddpr_data_info <- 
  read_rds(
    file = here::here("manuscript", "benchmarking", "ddpr_data_info.rds")
  )

ddpr_data_info
```

```{r find_largest_ddpr_cohort}
largest_cohort_names <- 
  ddpr_data_info %>% 
  filter(num_files == 288L) %>% 
  pull(file_names) %>% 
  pluck(1)

largest_cohort_paths <- 
  file.path(ddpr_path, largest_cohort_names)

ddpr_5 <- largest_cohort_paths[1:5]

ddpr_10 <- largest_cohort_paths[1:10]

ddpr_25 <- largest_cohort_paths[1:25]

ddpr_50 <- largest_cohort_paths[1:50]

ddpr_100 <- largest_cohort_paths[1:100]
```

```{r create_ddpr_datasets}
# put together a benchmarking dataset that includes a set of 5, 10, 25, 
# 50, and 100 FCS files from the largest ddpr cohort
ddpr_datasets <- 
  tibble(
    num_files = c(5, 10, 25, 50, 100)
  ) %>% 
  mutate(
    file_paths = list(ddpr_5, ddpr_10, ddpr_25, ddpr_50, ddpr_100),
    num_cells = map_int(.x = file_paths, .f = ~ nrow(tof_read_data(.x)))
  )

ddpr_datasets
```


```{r create_ddpr_datasets_mini}
# put together a smaller benchmarking dataset that includes a set of 1 to 
# 20 FCS files from the largest ddpr cohort
ddpr_datasets_mini <- 
  tibble(
    num_files = 1:20, 
    file_paths = map(.x = num_files, .f = ~ largest_cohort_paths[1:.x])
  ) %>% 
  mutate(num_cells = map_int(.x = file_paths, .f = ~nrow(tof_read_data(.x))))

ddpr_datasets_mini
```


```{r set_up_csvs, eval = FALSE}
# take the largest ddpr cohort and convert all files to CSVs, then save in a 
# new directory on Box
ddpr_large <- 
  largest_cohort_paths %>% 
  tof_read_data()

ddpr_large %>% 
  mutate(file_name = str_remove(string = file_name, pattern = "\\.fcs")) %>% 
  tof_write_data(
    group_cols = file_name, 
    out_path = csv_path, 
    format = "csv"
  )
```



```{r create_preread_tibbles_and_flowsets}
# set up pre-read data sets for downsampling, _____. 

## every file in the 20-FCS dataset is a row, with single-cell data stored 
## in the "data" column
ddpr_mini_tibble <- 
  tof_read_data(ddpr_datasets_mini$file_paths[[nrow(ddpr_datasets_mini)]]) %>% 
  group_by(file_name) %>% 
  nest() %>% 
  ungroup()

## same as above, but in flowSet form (flowSet of 20 experiments)
ddpr_mini_flowset <- 
  flowCore::read.flowSet(files = ddpr_datasets_mini$file_paths[[nrow(ddpr_datasets_mini)]])

# store each benchmark iteration as a single row in a tibble. 
# Each row is an iteration. The "tibbles" column contains a single tibble 
# with all the cells for that fold. The "flowSets" column contains a single 
# flowSet with all the cells for that fold.
ddpr_data_mini <- 
  ddpr_datasets_mini %>% 
  mutate(
    tibbles = 
      map(
        .x = 1:nrow(ddpr_datasets_mini), 
        .f = ~ ddpr_mini_tibble %>% 
          slice_head(n = .x) %>% 
          unnest(cols = data)
        ), 
    flowSets = 
      map(
        .x = 1:nrow(ddpr_datasets_mini), 
        .f = ~ddpr_mini_flowset[1:.x]
      )
  )

```

```{r store_all_file_paths}
# handle the file paths for saving/loading benchmarking results
in_out_paths <- 
  tibble(
    process = 
      c("fcs", "csv", "downsampling", "preprocessing", "tsne", "pca", "umap",  "flowsom", "extraction"), 
    pattern = 
      paste0(
        c(
          "io_benchmarking_speed_", 
          "io_benchmarking_speed_csv_", 
          "downsample_benchmarking_speed_", 
          "preprocess_benchmarking_speed_", 
          "tsne_benchmarking_speed_", 
          "pca_benchmarking_speed_", 
          "umap_benchmarking_speed_", 
          "flowsom_benchmarking_speed_", 
          "extract_benchmarking_speed_"
        ), 
        "[:digit:]+"
      ), 
    largest_size = 
      map_chr(
        .x = pattern, 
        .f = ~
          here::here("manuscript", "benchmarking") %>% 
          dir() %>% 
          str_extract(.x) %>% 
          na.omit() %>% 
          str_extract("[:digit:]+$") %>% 
          unique() %>% 
          sort() %>% 
          pluck(1), 
      ), 
    file_name = 
      paste0(
        str_replace(
          string = pattern, pattern = "\\[.+\\]\\+", 
          as.character(num_repeats)
        ),
        ".rds"
      ),
    alternate_name = 
      paste0(
        str_replace(string = pattern, pattern = "\\[.+\\]\\+", largest_size),
        ".rds"
      )
  )
```


## Reading data

```{r read_data_functions, eval = FALSE}
### {tidytof} function
read_data_tidytof <- function(file_names) { 
  return(tof_read_data(file_names))
}

### Base R function
read_data_base <- function(file_names) { 
  extension_type <- tools::file_ext(file_names[[1]])
  if (extension_type == "csv") { 
    data_list <- lapply(X = file_names, read.csv)
    output_data <- rbind(data_list)
  } else { 
    flowset <- read.flowSet(file_names)
    raw_exprs <- fsApply(flowset, exprs)
    output_data <- as.data.frame(raw_exprs)
    channel_names <- as.character(flowset[[1]]@parameters[["desc"]])
    metal_names <- as.character(flowset[[1]]@parameters[["name"]])
    metals_to_change <- grepl(pattern = "Di", x = metal_names)
    metal_names[metals_to_change] <- 
      gsub(
        pattern = "\\(|\\)|(Di)",
        replacement = "", 
        x = metal_names[metals_to_change]
      )
    new_names <- paste(channel_names, metal_names, sep = "_")
    new_names[!metals_to_change] <- channel_names[!metals_to_change]
    colnames(output_data) <- new_names
  }
  return(output_data)
}

### flowcore function
read_data_flowcore <- function(file_names) { 
  flowset <- read.flowSet(file_names)
  channel_names <- as.character(flowset[[1]]@parameters[["desc"]])
  metal_names <- as.character(flowset[[1]]@parameters[["name"]])
  metals_to_change <- grepl(pattern = "Di", x = metal_names)
  metal_names[metals_to_change] <- 
    gsub(
      pattern = "\\(|\\)|(Di)",
      replacement = "", 
      x = metal_names[metals_to_change]
    )
  new_names <- paste(channel_names, metal_names, sep = "_")
  new_names[!metals_to_change] <- channel_names[!metals_to_change]
  num_files <- length(flowset)
  flowframe_list <- list()
  for (i in 1:num_files) {
    flowframe <- flowset[[i]]
    flowframe@parameters[["desc"]] <- new_names
    flowframe@parameters[["name"]] <- new_names
    flowframe_list[[i]] <- flowframe
  }
  result <- flowSet(flowframe_list)
  return(result)
}
```


```{r io_benchmarking_function}
### Benchmarking function
benchmark_io <- 
  function(file_paths) { 
    microbenchmark(
      read_data_tidytof(file_names = file_paths), 
      read_data_base(file_names = file_paths),
      read_data_flowcore(file_names = file_paths),
      times = num_repeats, 
      unit = "s" # seconds
    ) %>% 
      as_tibble() %>% 
      transmute(
        engine = if_else(str_detect(expr, "tidytof"), "tidytof", "base"), 
        time = time / (10^9)
      )
  }

benchmark_io_csv <- 
  function(file_paths) { 
    microbenchmark(
      read_data_tidytof(file_names = file_paths), 
      read_data_base(file_names = file_paths),
      times = num_repeats, 
      unit = "s" # seconds
    ) %>% 
      as_tibble() %>% 
      transmute(
        engine = if_else(str_detect(expr, "tidytof"), "tidytof", "base"), 
        time = time / (10^9)
      )
  }

# file paths
file_name <- 
  in_out_paths %>% 
  filter(process == "fcs") %>% 
  pull(file_name)

alternate_file_name <- 
  in_out_paths %>% 
  filter(process == "fcs") %>% 
  pull(alternate_name)

file_name_csv <- 
  in_out_paths %>% 
  filter(process == "csv") %>% 
  pull(file_name)

alternate_file_name_csv <- 
  in_out_paths %>% 
  filter(process == "csv") %>% 
  pull(alternate_name)
```

```{r benchmark_io, eval = run_io}
### Perform benchmarking
io_benchmarking <- 
  ddpr_datasets_mini %>% 
  mutate(io_benchmarks = map(.x = file_paths, .f = benchmark_io))

io_benchmarking_csv <- 
  ddpr_datasets_mini %>% 
  mutate(
    file_paths = 
      map(.x = file_paths, .f = str_replace, pattern = "\\.fcs", replacement = ".csv") %>% 
      map(.f = str_replace, pattern = "DDPR_Data", replacement = "DDPR_Data_csv") 
  ) %>% 
  mutate(io_benchmarks = map(.x = file_paths, .f = benchmark_io_csv))
```


```{r save_io_result, eval = run_io}
# save benchmarking output as an .rds
io_benchmarking %>% 
  write_rds(file = here::here("manuscript", "benchmarking", file_name))
```

```{r visualize_io_result, message = FALSE}
if (!file.exists(here::here("manuscript", "benchmarking", file_name))) { 
  file_name <- alternate_file_name
}

io_benchmarking <- 
  read_rds(file = here::here("manuscript", "benchmarking", file_name))

engines <- 
  io_benchmarking %>% 
  select(io_benchmarks) %>% 
  unnest(io_benchmarks) %>% 
  mutate(engine = if_else(engine == "base", "flowcore", engine)) %>% 
  pull(engine) %>% 
  unique()

num_engines <- length(engines)

my_palette <- charizard[which(names(charizard) %in% engines)]

max_cells <- max(io_benchmarking$num_cells)
width <- max_cells * 0.03
  

# make a plot to summarize 
io_speed_plot <- 
  io_benchmarking %>% 
  select(num_files, num_cells, io_benchmarks) %>% 
  unnest(cols = io_benchmarks) %>% 
  mutate(engine = if_else(engine == "base", "flowcore", engine)) %>% 
  group_by(num_files, num_cells, engine) %>% 
  summarize(
    mean_time = median(time), 
    max_time = quantile(time, 0.75), 
    min_time = quantile(time, 0.25)
  ) %>% 
  ungroup() %>% 
  ggplot(
    aes(
      x = num_cells, 
      y = mean_time, 
      fill = engine, 
      color = engine, 
      ymin = min_time, 
      ymax = max_time
    )
  ) + 
  geom_errorbar(width = width) + 
  geom_line(aes(group = engine)) + 
  geom_point(shape = 21, size = 2, color = "black") + 
  scale_x_continuous(labels = scales::label_scientific()) + 
  scale_fill_manual(values = my_palette) + 
  scale_color_manual(values = my_palette) +
  theme_bw() + 
  labs(
    subtitle = "FCS reader speed benchmarking",
    x = "Number of cells", 
    y = "Time (seconds)", 
    fill = NULL, 
    color = NULL
  )

io_speed_plot

io_speed_plot %>% 
  ggsave(
    filename = 
      here::here(
        "manuscript", 
        "benchmarking", 
        str_replace(file_name, ".rds", ".pdf")
      ), 
    device = "pdf", 
    width = 5, 
    height = 3
  )

```

```{r save_io_result_csv, eval = FALSE}
# save benchmarking results as an .rds
io_benchmarking_csv %>% 
  write_rds(file = here::here("manuscript", "benchmarking", file_name_csv))
```

```{r visualize_io_result_csv, message = FALSE}
if (!file.exists(here::here("manuscript", "benchmarking", file_name_csv))) { 
  file_name_csv <- alternate_file_name_csv
}

io_benchmarking_csv <- 
  read_rds(file = here::here("manuscript", "benchmarking", file_name_csv))

num_engines <- 
  io_benchmarking_csv %>% 
  select(io_benchmarks) %>% 
  unnest(io_benchmarks) %>% 
  pull(engine) %>% 
  unique() %>% 
  length()

max_cells <- max(io_benchmarking_csv$num_cells)
width <- max_cells * 0.03

# visualize speed benchmarking results
io_speed_plot_csv <- 
  io_benchmarking_csv %>% 
  select(num_files, num_cells, io_benchmarks) %>% 
  unnest(cols = io_benchmarks) %>% 
  group_by(num_files, num_cells, engine) %>% 
  summarize(
    mean_time = median(time), 
    max_time = quantile(time, 0.75), 
    min_time = quantile(time, 0.25)
  ) %>% 
  ungroup() %>% 
  ggplot(
    aes(
      x = num_cells, 
      y = mean_time, 
      fill = engine, 
      color = engine, 
      ymin = min_time, 
      ymax = max_time
    )
  ) + 
  geom_errorbar(width = width) + 
  geom_line(aes(group = engine)) + 
  geom_point(shape = 21, size = 2, color = "black") + 
  scale_x_continuous(labels = scales::label_scientific()) + 
  scale_fill_manual(values = charizard[1:num_engines]) + 
  scale_color_manual(values = charizard[1:num_engines]) +
  theme_bw() + 
  labs(
    subtitle = "CSV reader speed benchmarking",
    x = "Number of cells", 
    y = "Time (seconds)", 
    fill = NULL, 
    color = NULL
  )

io_speed_plot_csv

io_speed_plot_csv %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", str_replace(file_name_csv, ".rds", ".pdf")), 
    device = "pdf", 
    width = 5, 
    height = 3
  )
```

## Downsampling

```{r downsample_functions, eval = FALSE}
# tidytof
downsample_tidytof <- 
  function(data_frame) {
    result <- data_frame %>% 
      tof_downsample_constant(group_cols = file_name, num_cells = 200)
    return(result)
  }

# base R
downsample_base <- 
  function(data_frame) {
    file_names <- unique(data_frame$file_name)
    num_file_names <- length(file_names)
    final_subset <- list()
    for (i in 1:num_file_names) { 
      file_name <- file_names[[i]]
      file_indices <- which(data_frame$file_name == file_name)
      subset_indices <- sample(x = file_indices, size = 200L)
      final_subset[[file_name]] <- subset_indices
    }
    final_subset <- sort(as.numeric(c(final_subset, recursive = TRUE)))
    result <- data_frame[final_subset, ]
    return(result)
  }

# flowcore
downsample_flowcore <- 
  function(flowset) { 
    subset_flowframe <- function(flowframe) {
      num_cells <- nrow(flowframe)
      sample_indices <- sort(sample(x = 1:num_cells, size = 200L))
      subsampled_flowframe <- flowframe[sample_indices]
      return(subsampled_flowframe)
    }
    subsampled_flowset <- flowCore::fsApply(x = flowset, FUN = subset_flowframe)
  }
```


```{r downsample_benchmark_function}
### Benchmarking function
benchmark_downsample <- 
  function(data_frame, flowset) { 
    microbenchmark(
      downsample_tidytof(data_frame), 
      downsample_base(data_frame),
      downsample_flowcore(flowset),
      times = num_repeats, 
      unit = "s" # seconds
    ) %>% 
      as_tibble() %>% 
      transmute(
        engine = 
          case_when(
            str_detect(expr, "tidytof")  ~ "tidytof",
            str_detect(expr, "flowcore") ~ "flowcore", 
            TRUE                         ~ "base"
          ), 
        time = time / (10^9)
      )
  }

file_name <- 
  in_out_paths %>% 
  filter(process == "downsampling") %>% 
  pull(file_name)

alternate_file_name <- 
  in_out_paths %>% 
  filter(process == "downsampling") %>% 
  pull(alternate_name)
```


### Perform benchmarking

```{r benchmark_downsampling, eval = run_downsample}
downsample_benchmarking <- 
  ddpr_data_mini %>% 
  mutate(
    downsample_benchmarks = 
      map2(
        .x = tibbles, 
        .y = flowSets, 
        .f = benchmark_downsample
      )
  ) %>% 
  select(-tibbles, -flowSets)
```

```{r save_downsampling_result, eval = run_downsample}
# save benchmarking output as an .rds
downsample_benchmarking %>% 
  write_rds(file = here::here("manuscript", "benchmarking", file_name))
```

```{r visualize_downsampling_result, message = FALSE}
if (!file.exists(here::here("manuscript", "benchmarking", file_name))) { 
  file_name <- alternate_file_name
}

downsample_benchmarking <- 
  read_rds(file = here::here("manuscript", "benchmarking", file_name))

max_cells <- max(downsample_benchmarking$num_cells)
width <- max_cells * 0.03

# make a plot to summarize 
downsample_speed_plot <- 
  downsample_benchmarking %>% 
  select(num_files, num_cells, downsample_benchmarks) %>% 
  unnest(cols = downsample_benchmarks) %>% 
  group_by(num_files, num_cells, engine) %>% 
  summarize(
    mean_time = median(time), 
    max = max(time), 
    min = min(time),
    sd_time = sd(time),
    max_time = quantile(time, 0.75), 
    min_time = quantile(time, 0.25)
  ) %>%
  ungroup() %>%
  ggplot(
    aes(
      x = num_cells, 
      y = mean_time, 
      fill = engine, 
      color = engine, 
      ymin = min_time, 
      ymax = max_time
    )
  ) + 
  geom_errorbar(width = width) +
  geom_line(aes(group = engine)) + 
  geom_point(shape = 21, size = 2, color = "black") + 
  scale_x_continuous(labels = scales::label_scientific()) + 
  scale_color_manual(values = charizard) + 
  scale_fill_manual(values = charizard) +
  theme_bw() + 
  labs(
    subtitle = "Downsampling speed benchmarking",
    x = "Number of cells", 
    y = "Time (seconds)", 
    fill = NULL, 
    color = NULL
  )

downsample_speed_plot

downsample_speed_plot %>% 
  ggsave(
    filename = 
      here::here(
        "manuscript", 
        "benchmarking", 
        str_replace(file_name, ".rds", ".pdf")
      ), 
    device = "pdf", 
    width = 5, 
    height = 3
  )
```

## Preprocessing

```{r preprocess_functions, eval = FALSE}
# tidytof function
preprocess_tidytof <- 
  function(data_frame) {
    return(tof_preprocess(data_frame, undo_noise = FALSE))
  }

# preprocess_tidytof(data_frame = ddpr_data_mini$tibbles[[2]])

# base R function
preprocess_base <- 
  function(data_frame) {
    num_cols <- ncol(data_frame)
    for (i in 1:num_cols) { 
      col_values <- data_frame[[i]]
      if (is.numeric(col_values)) { 
        new_values <- asinh(col_values / 5)
        data_frame[[i]] <- new_values
      }
    }
    return(data_frame)
  }

# flowcore
preprocess_flowcore <- 
  function(flowset) {
    numeric_columns <- 
      as.logical(apply(X = exprs(flowset[[1]]), MARGIN = 2, FUN = is.numeric))
    asinh_transform <- arcsinhTransform(a = 0, b = 0.2, c = 0)
    translist <- transformList(colnames(flowset)[numeric_columns],  asinh_transform)
    preprocessed_flowset <- transform(flowset, translist)
    return(preprocessed_flowset)
  }

```


```{r preprocess_benchmarking_function}
benchmark_preprocess <- 
  function(data_frame, flowset) { 
    microbenchmark(
      preprocess_tidytof(data_frame), 
      preprocess_base(data_frame),
      preprocess_flowcore(flowset),
      times = num_repeats, 
      unit = "s" # seconds
    ) %>% 
      as_tibble() %>% 
      transmute(
        engine = 
          case_when(
            str_detect(expr, "tidytof")  ~ "tidytof",
            str_detect(expr, "flowcore") ~ "flowcore", 
            TRUE                         ~ "base"
          ), 
        time = time / (10^9)
      )
  }

file_name <- 
  in_out_paths %>% 
  filter(process == "preprocessing") %>% 
  pull(file_name)

alternate_file_name <- 
  in_out_paths %>% 
  filter(process == "preprocessing") %>% 
  pull(alternate_name)
```



```{r benchmark_preprocessing, eval = run_preprocess}
### Perform benchmarking
preprocess_benchmarking <- 
  ddpr_data_mini %>% 
  mutate(preprocess_benchmarks = map2(.x = tibbles, .y = flowSets, .f = benchmark_preprocess)) %>% 
  select(-tibbles, -flowSets)
```


```{r save_benchmarking_result, eval = run_preprocess}
# save benchmarking output as an .rds
preprocess_benchmarking %>% 
  write_rds(file = here::here("manuscript", "benchmarking", file_name))
```

```{r visualize_benchmarking_result, message = FALSE}
# visualize results
if (!file.exists(here::here("manuscript", "benchmarking", file_name))) { 
  file_name <- alternate_file_name
}

preprocess_benchmarking <- 
  read_rds(file = here::here("manuscript", "benchmarking", file_name))

max_cells <- max(preprocess_benchmarking$num_cells)
width <- max_cells * 0.03

# make a plot to summarize 
preprocess_speed_plot <- 
  preprocess_benchmarking %>% 
  select(num_files, num_cells, preprocess_benchmarks) %>% 
  unnest(cols = preprocess_benchmarks) %>% 
  group_by(num_files, num_cells, engine) %>% 
  summarize(
    mean_time = median(time), 
    max_time = quantile(time, 0.75), 
    min_time = quantile(time, 0.25)
  ) %>% 
  ungroup() %>% 
  ggplot(
    aes(
      x = num_cells, 
      y = mean_time, 
      fill = engine, 
      color = engine, 
      ymin = min_time, 
      ymax = max_time
    )
  ) + 
  geom_errorbar(width = width) +
  geom_line(aes(group = engine)) + 
  geom_point(
    shape = 21, 
    size = 2, 
    color = "black", 
    position = position_dodge(width = 10000)
  ) + 
  scale_x_continuous(labels = scales::label_scientific()) + 
  scale_color_manual(values = charizard) + 
  scale_fill_manual(values = charizard) + 
  theme_bw() + 
  labs(
    subtitle = "Preprocessing speed benchmarking",
    x = "Number of cells", 
    y = "Time (seconds)", 
    fill = NULL, 
    color = NULL
  )

preprocess_speed_plot

preprocess_speed_plot %>% 
  ggsave(
    filename = 
      here::here(
        "manuscript", 
        "benchmarking", 
        str_replace(file_name, ".rds", ".pdf")
      ), 
    device = "pdf", 
    width = 5, 
    height = 3
  )
```

## Dimensionality reduction

```{r setup_dimensionality_reduction_data}
# setup a nested tibble in which each row is a number of sampled cells 
# and each entry in the "data_frames" column is a tof_tbl containing 
# that many cells. 
# These tibbles are to be used for the dimensionality reduction benchmarking.

# pca 
pca_tibbles <- 
  tibble(
    sample_cells = (seq(1000, 10^6, length.out = 10)), 
    data_frames = 
      map(
        .x = sample_cells, 
        .f = ~ unnest(ddpr_mini_tibble, cols = data) %>% 
          tof_downsample_constant(num_cells = .x) %>% 
          tof_preprocess(undo_noise = FALSE)
      )
  )

flowset_path <- file.path(base_path, "tidytof_pca_fcs_files")

pca_tibbles %>% 
  select(sample_cells, data_frames) %>% 
  unnest(cols = data_frames) %>% 
  mutate(file_name = str_remove(file_name, "\\.fcs")) %>% 
  tof_write_data(
    group_cols = c(sample_cells, file_name), 
    out_path = flowset_path, 
    format = "fcs"
  )

pca_flowsets <- 
  tibble(
    num_cells = pca_tibbles$sample_cells
  ) %>% 
  mutate(
    file_paths = 
      map(.x = as.character(num_cells), .f = ~ dir(flowset_path, pattern = .x, full.names = TRUE))
  )

pca_flowsets$file_paths[nrow(pca_flowsets)] <- 
  list(dir(flowset_path, pattern = "1e+", full.names = TRUE))
  
pca_flowsets <- 
  pca_flowsets %>% 
  mutate(
    flowSets = map(.x = file_paths, .f = read.flowSet)
  )

pca_tibbles$pca_flowSets <- pca_flowsets$flowSets

# tsne
tsne_tibbles <- 
  tibble(
    sample_cells = (seq(1000, 10^4, length.out = 10)), 
    data_frames = 
      map(
        .x = sample_cells, 
        .f = ~ unnest(ddpr_mini_tibble, cols = data) %>% 
          tof_downsample_constant(num_cells = .x) %>% 
          tof_preprocess(undo_noise = FALSE)
      )
  )

flowset_path <- file.path(base_path, "tidytof_tsne_fcs_files")

tsne_tibbles %>% 
  select(sample_cells, data_frames) %>% 
  unnest(cols = data_frames) %>% 
  mutate(file_name = str_remove(file_name, "\\.fcs")) %>% 
  tof_write_data(
    group_cols = c(sample_cells, file_name), 
    out_path = flowset_path, 
    format = "fcs"
  )

tsne_flowsets <- 
  tibble(
    num_cells = tsne_tibbles$sample_cells
  ) %>% 
  mutate(
    file_paths = 
      map(.x = paste0(as.character(num_cells), "_"), .f = ~ dir(flowset_path, pattern = .x, full.names = TRUE))
  )

tsne_flowsets <- 
  tsne_flowsets %>% 
  mutate(
    flowSets = map(.x = file_paths, .f = read.flowSet)
  )

tsne_tibbles$tsne_flowSets <- tsne_flowsets$flowSets

# peek
pca_tibbles %>% 
  slice_head(n = 2)

tsne_tibbles %>% 
  slice_head(n = 2)
```


```{r dimensionality_reduction_functions}
### {tidytof} function
tsne_tidytof <- 
  function(data_frame) {
    return(tof_reduce_tsne(data_frame, tsne_cols = starts_with("CD")))
  }

# tsne_tidytof(data_frame = ddpr_data_mini$tibbles[[2]][1:500,])

pca_tidytof <- 
  function(data_frame) {
    return(tof_reduce_pca(data_frame, pca_cols = starts_with("CD"), num_comp = 2))
  }

umap_tidytof <- 
  function(data_frame) { 
    return(tof_reduce_umap(data_frame, umap_cols = starts_with("CD")))
  }

# pca_tidytof(data_frame = ddpr_data_mini$tibbles[[2]][1:500,])

# base R functions
tsne_base <- 
  function(data_frame) {
    dr_colnames <- grepl(pattern = "^CD", x = colnames(data_frame))
    tsne_df <- data_frame[, dr_colnames]
    tsne_result <- Rtsne::Rtsne(X = as.matrix(tsne_df))
    tsne_embeddings <- tsne_result$Y
    final_result <- 
      data.frame(tsne_1 = tsne_embeddings[, 1], tsne_2 = tsne_embeddings[, 2])
    return(final_result)
  }

#tsne_base(data_frame = ddpr_data_mini$tibbles[[2]][1:500,])
pca_base <- 
  function(data_frame) {
    dr_colnames <- grepl(pattern = "^CD", x = colnames(data_frame))
    pca_df <- data_frame[, dr_colnames]
    column_variances <- 
      apply(X = pca_df, MARGIN = 2, FUN = function(x) length(unique(x)))
    zv_columns <- as.logical(round(column_variances) == 1)
    pca_df <- pca_df[, !zv_columns]
    pca_result <- prcomp(x = as.matrix(pca_df), center = TRUE, scale. = TRUE)
    pca_embeddings <- pca_result$x
    final_result <- 
      data.frame(PC1 = pca_embeddings[, 1], PC2 = pca_embeddings[, 2])
    return(final_result)
  }

umap_base <- 
  function(data_frame) {
    dr_colnames <- grepl(pattern = "^CD", x = colnames(data_frame))
    umap_df <- data_frame[, dr_colnames]
    column_variances <- 
      apply(X = umap_df, MARGIN = 2, FUN = function(x) length(unique(x)))
    zv_columns <- as.logical(round(column_variances) == 1)
    umap_df <- umap_df[, !zv_columns]
    umap_result <- 
      uwot::umap(X = umap_df, n_neighbors = 5, scale = TRUE)
    final_result <- 
      data.frame(UMAP1 = umap_result[, 1], UMAP2 = umap_result[, 2])
    return(final_result)
  }

#pca_base(data_frame = ddpr_data_mini$tibbles[[2]][1:500,])

tsne_flowcore <- 
  function(flowset) {
    channel_names <- flowset[[1]]@parameters@data$desc
    dr_columns <- grepl(pattern = "^CD", x = channel_names)
    my_exprs <- fsApply(x = flowset, FUN = exprs)
    dr_exprs <- my_exprs[, dr_columns]
    tsne_result <- Rtsne::Rtsne(X = as.matrix(dr_exprs))
    tsne_embeddings <- tsne_result$Y
    colnames(tsne_embeddings) <- paste0(".tsne", 1:2)
    flowframe_num_cells <- as.numeric(fsApply(x = flowset, FUN = nrow))
    starting_index <- 1L
    flowframe_list <- list()
    for (i in 1:length(flowframe_num_cells)) { 
      flowframe <- flowset[[i]]
      num_cells <- flowframe_num_cells[[i]]
      ending_index <- starting_index + num_cells - 1
      flowframe_tsne <- tsne_embeddings[starting_index:ending_index , ]
      new_flowframe <- fr_append_cols(flowframe, flowframe_tsne)
      flowframe_list[[i]] <- new_flowframe
      starting_index = ending_index + 1
    }
    result <- flowSet(flowframe_list)
    return(result)
  }

pca_flowcore <- function(flowset) { 
  channel_names <- flowset[[1]]@parameters@data$desc
    dr_columns <- grepl(pattern = "^CD", x = channel_names)
    my_exprs <- fsApply(x = flowset, FUN = exprs)
    dr_exprs <- my_exprs[, dr_columns]
    column_variances <- 
      apply(X = dr_exprs, MARGIN = 2, FUN = function(x) length(unique(x)))
    zv_columns <- as.logical(round(column_variances) == 1)
    dr_exprs <- dr_exprs[, !zv_columns]
    pca_result <- prcomp(x = as.matrix(dr_exprs), center = TRUE, scale. = TRUE)
    pca_embeddings <- pca_result$x[, 1:2]
    colnames(pca_embeddings) <- paste0("PC", 1:2)
    flowframe_num_cells <- as.numeric(fsApply(x = flowset, FUN = nrow))
    starting_index <- 1L
    flowframe_list <- list()
    for (i in 1:length(flowframe_num_cells)) { 
      flowframe <- flowset[[i]]
      num_cells <- flowframe_num_cells[[i]]
      ending_index <- starting_index + num_cells - 1
      flowframe_pca <- pca_embeddings[starting_index:ending_index , ]
      new_flowframe <- fr_append_cols(flowframe, flowframe_pca)
      flowframe_list[[i]] <- new_flowframe
      starting_index = ending_index + 1
    }
    result <- flowSet(flowframe_list)
    return(result)
}

umap_flowcore <- function(flowset) { 
  channel_names <- flowset[[1]]@parameters@data$desc
    dr_columns <- grepl(pattern = "^CD", x = channel_names)
    my_exprs <- fsApply(x = flowset, FUN = exprs)
    dr_exprs <- my_exprs[, dr_columns]
    column_variances <- 
      apply(X = dr_exprs, MARGIN = 2, FUN = function(x) length(unique(x)))
    zv_columns <- as.logical(round(column_variances) == 1)
    dr_exprs <- dr_exprs[, !zv_columns]
    umap_result <- 
      uwot::umap(X = dr_exprs, n_neighbors = 5, scale = TRUE)    
    colnames(umap_result) <- paste0("UMAP", 1:2)
    flowframe_num_cells <- as.numeric(fsApply(x = flowset, FUN = nrow))
    starting_index <- 1L
    flowframe_list <- list()
    for (i in 1:length(flowframe_num_cells)) { 
      flowframe <- flowset[[i]]
      num_cells <- flowframe_num_cells[[i]]
      ending_index <- starting_index + num_cells - 1
      flowframe_umap <- umap_result[starting_index:ending_index , ]
      new_flowframe <- fr_append_cols(flowframe, flowframe_umap)
      flowframe_list[[i]] <- new_flowframe
      starting_index = ending_index + 1
    }
    result <- flowSet(flowframe_list)
    return(result)
  }

```



```{r dimensionality_reduction_benchmarking_functions}
### Benchmarking function
benchmark_tsne <- 
  function(data_frame, flowset) { 
    microbenchmark(
      tsne_tidytof(data_frame), 
      tsne_base(data_frame),
      tsne_flowcore(flowset),
      times = num_repeats, 
      unit = "s" # seconds
    ) %>% 
      as_tibble() %>% 
       transmute(
        engine = 
          case_when(
            str_detect(expr, "tidytof")  ~ "tidytof",
            str_detect(expr, "flowcore") ~ "flowcore", 
            TRUE                         ~ "base"
          ), 
        time = time / (10^9)
      )
  }

file_name_tsne <- 
  in_out_paths %>% 
  filter(process == "tsne") %>% 
  pull(file_name)

alternate_file_name_tsne <- 
  in_out_paths %>% 
  filter(process == "tsne") %>% 
  pull(alternate_name)

benchmark_pca <- 
  function(data_frame, flowset) { 
    microbenchmark(
      pca_tidytof(data_frame), 
      pca_base(data_frame),
      pca_flowcore(flowset),
      times = num_repeats, 
      unit = "s" # seconds
    ) %>% 
      as_tibble() %>% 
      transmute(
        engine = 
          case_when(
            str_detect(expr, "tidytof")  ~ "tidytof",
            str_detect(expr, "flowcore") ~ "flowcore", 
            TRUE                         ~ "base"
          ), 
        time = time / (10^9)
      )
  }

file_name_pca <- 
  in_out_paths %>% 
  filter(process == "pca") %>% 
  pull(file_name)

alternate_file_name_pca <- 
  in_out_paths %>% 
  filter(process == "pca") %>% 
  pull(alternate_name)

# umap
benchmark_umap <- 
  function(data_frame, flowset) { 
    microbenchmark(
      umap_tidytof(data_frame), 
      umap_base(data_frame),
      umap_flowcore(flowset),
      times = num_repeats, 
      unit = "s" # seconds
    ) %>% 
      as_tibble() %>% 
       transmute(
        engine = 
          case_when(
            str_detect(expr, "tidytof")  ~ "tidytof",
            str_detect(expr, "flowcore") ~ "flowcore", 
            TRUE                         ~ "base"
          ), 
        time = time / (10^9)
      )
  }

file_name_umap <- 
  in_out_paths %>% 
  filter(process == "umap") %>% 
  pull(file_name)

alternate_file_name_umap <- 
  in_out_paths %>% 
  filter(process == "umap") %>% 
  pull(alternate_name)

```


```{r benchmark_tsne, eval = run_tsne}
### Perform benchmarking
tsne_benchmark <- 
  tsne_tibbles %>%
  mutate(
    tsne_benchmarks = map2(.x = data_frames, .y = tsne_flowSets, .f = benchmark_tsne)
  ) %>% 
  select(-data_frames, - tsne_flowSets) %>% 
  rename(num_cells = sample_cells)

tsne_benchmark
```


```{r benchmark_pca, eval = run_pca}
pca_benchmark <- 
  pca_tibbles %>%
  mutate(
    pca_benchmarks = map2(.x = data_frames, .y = pca_flowSets, .f = benchmark_pca)
  ) %>% 
  select(-data_frames, -pca_flowSets) %>% 
  rename(num_cells = sample_cells)
```

```{r benchmark_umap, eval = run_umap}
### Perform benchmarking
umap_benchmark <- 
  tsne_tibbles %>%
  mutate(
    umap_benchmarks = map2(.x = data_frames, .y = tsne_flowSets, .f = benchmark_umap)
  ) %>% 
  select(-data_frames, - tsne_flowSets) %>% 
  rename(num_cells = sample_cells)

umap_benchmark
```


```{r save_tsne_result, eval = run_tsne}
# save benchmarking output as an .rds
tsne_benchmark %>% 
  write_rds(file = here::here("manuscript", "benchmarking", file_name_tsne))
```

```{r visualize_tsne_result, message = FALSE}
if (!file.exists(here::here("manuscript", "benchmarking", file_name_tsne))) { 
  file_name_tsne <- alternate_file_name_tsne
}

tsne_benchmark <- 
  read_rds(file = here::here("manuscript", "benchmarking", file_name_tsne))

max_cells <- max(tsne_benchmark$num_cells)
width <- max_cells * 0.03
  
engines <- 
  tsne_benchmark %>% 
  select(tsne_benchmarks) %>% 
  unnest(tsne_benchmarks) %>% 
  pull(engine) %>% 
  unique()

num_engines <- length(engines)

my_palette <- charizard[which(names(charizard) %in% engines)]

# make a plot to summarize 
tsne_speed_plot <- 
  tsne_benchmark %>%
  select(num_cells, tsne_benchmarks) %>% 
  unnest(cols = tsne_benchmarks) %>% 
  group_by(num_cells, engine) %>% 
  summarize(
    mean_time = mean(time), 
    max_time = max(time), 
    min_time = min(time)
  ) %>% 
  ungroup() %>% 
  ggplot(
    aes(
      x = num_cells, 
      y = mean_time, 
      fill = engine, 
      color = engine, 
      ymin = min_time, 
      ymax = max_time
    )
  ) + 
  geom_errorbar(width = width) + 
  geom_line(aes(group = engine)) + 
  geom_point(shape = 21, size = 2, color = "black") + 
  scale_x_continuous(labels = scales::label_scientific()) + 
  scale_color_manual(values = my_palette) + 
  scale_fill_manual(values = my_palette) +
  theme_bw() + 
  labs(
    subtitle = "tSNE speed benchmarking",
    x = "Number of cells", 
    y = "Time (seconds)", 
    fill = NULL, 
    color = NULL
  )

tsne_speed_plot

tsne_speed_plot %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", str_replace(file_name_tsne, ".rds", ".pdf")), 
    device = "pdf", 
    width = 5, 
    height = 3
  )

```


```{r save_pca_result, eval = run_pca}
# save benchmarking output as an .rds
pca_benchmark %>% 
  write_rds(file = here::here("manuscript", "benchmarking", file_name_pca))
```


```{r visualize_pca_result, message = FALSE}
if (!file.exists(here::here("manuscript", "benchmarking", file_name_pca))) { 
  file_name_pca <- alternate_file_name_pca
}

pca_benchmark <- 
  read_rds(file = here::here("manuscript", "benchmarking", file_name_pca))

max_cells <- max(pca_benchmark$num_cells)
width <- max_cells * 0.03
  
engines <- 
  pca_benchmark %>% 
  select(pca_benchmarks) %>% 
  unnest(pca_benchmarks) %>% 
  pull(engine) %>% 
  unique()

num_engines <- length(engines)

my_palette <- charizard[which(names(charizard) %in% engines)]

# make a plot to summarize 
pca_speed_plot <- 
  pca_benchmark %>%
  select(num_cells, pca_benchmarks) %>% 
  unnest(cols = pca_benchmarks) %>% 
  group_by(num_cells, engine) %>% 
  summarize(
    mean_time = median(time), 
    max_time = quantile(time, 0.75), 
    min_time = quantile(time, 0.25)
  ) %>% 
  ungroup() %>% 
  ggplot(
    aes(
      x = num_cells, 
      y = mean_time, 
      fill = engine, 
      color = engine, 
      ymin = min_time, 
      ymax = max_time
    )
  ) + 
  geom_errorbar(width = width) + 
  geom_line(aes(group = engine)) + 
  geom_point(shape = 21, size = 2, color = "black") + 
  scale_x_continuous(labels = scales::label_scientific()) + 
  scale_color_manual(values = my_palette) + 
  scale_fill_manual(values = my_palette) +
  theme_bw() + 
  labs(
    subtitle = "PCA speed benchmarking",
    x = "Number of cells", 
    y = "Time (seconds)", 
    fill = NULL, 
    color = NULL
  )

pca_speed_plot

pca_speed_plot %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", str_replace(file_name_pca, ".rds", ".pdf")), 
    device = "pdf", 
    width = 5, 
    height = 3
  )

```


```{r save_umap_result, eval = run_tsne}
# save benchmarking output as an .rds
umap_benchmark %>%
  write_rds(file = here::here("manuscript", "benchmarking", file_name_umap))
```

```{r visualize_umap_result, message = FALSE}
if (!file.exists(here::here("manuscript", "benchmarking", file_name_umap))) { 
  file_name_umap <- alternate_file_name_umap
}

umap_benchmark <- 
  read_rds(file = here::here("manuscript", "benchmarking", file_name_umap))

max_cells <- max(umap_benchmark$num_cells)
width <- max_cells * 0.03
  
engines <- 
  umap_benchmark %>% 
  select(umap_benchmarks) %>% 
  unnest(umap_benchmarks) %>% 
  pull(engine) %>% 
  unique()

num_engines <- length(engines)

my_palette <- charizard[which(names(charizard) %in% engines)]

# make a plot to summarize 
umap_speed_plot <- 
  umap_benchmark %>%
  select(num_cells, umap_benchmarks) %>% 
  unnest(cols = umap_benchmarks) %>% 
  group_by(num_cells, engine) %>% 
  summarize(
    mean_time = mean(time), 
    max_time = max(time), 
    min_time = min(time)
  ) %>% 
  ungroup() %>% 
  ggplot(
    aes(
      x = num_cells, 
      y = mean_time, 
      fill = engine, 
      color = engine, 
      ymin = min_time, 
      ymax = max_time
    )
  ) + 
  geom_errorbar(width = width) + 
  geom_line(aes(group = engine)) + 
  geom_point(shape = 21, size = 2, color = "black") + 
  scale_x_continuous(labels = scales::label_scientific()) + 
  scale_color_manual(values = my_palette) + 
  scale_fill_manual(values = my_palette) +
  theme_bw() + 
  labs(
    subtitle = "UMAP speed benchmarking",
    x = "Number of cells", 
    y = "Time (seconds)", 
    fill = NULL, 
    color = NULL
  )

umap_speed_plot

umap_speed_plot %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", str_replace(file_name_umap, ".rds", ".pdf")), 
    device = "pdf", 
    width = 5, 
    height = 3
  )
```

## Clustering

```{r flowsom_functions}
### {tidytof} function
flowsom_tidytof <-
  function(file_names) { 
    clusters <- 
      file_names %>% 
      tof_read_data() %>% 
      tof_preprocess() %>% 
      tof_cluster_flowsom(
        cluster_cols = starts_with("CD", ignore.case = FALSE), 
        perform_metaclustering = TRUE
      )
    return(clusters)
  }

### Base R function
flowsom_base <- 
  function(file_names) {
    flowset <- 
      flowCore::read.flowSet(
        files = file_names, 
        transformation = FALSE, 
        truncate_max_range = FALSE
      )
    raw_exprs <- flowCore::fsApply(flowset, flowCore::exprs, simplify = FALSE)
    asinh_exprs <- lapply(X = raw_exprs, FUN = function(x) asinh(x / 5))
    for (i in 1:length(asinh_exprs)) { 
      flowCore::exprs(flowset[[i]]) <- asinh_exprs[[i]]
    }
    channel_names <- as.character(flowset[[1]]@parameters[["desc"]])
    cluster_colnames <- 
      grepl(pattern = "^CD", x = channel_names, ignore.case = FALSE)
    flowsom_object <- FlowSOM::ReadInput(input = flowset)
    flowsom_som <- 
      FlowSOM::BuildSOM(fsom = flowsom_object, colsToUse = cluster_colnames)
    cluster_labels <- flowsom_som$map$mapping[, 1]
    metaclusters <- 
      FlowSOM::MetaClustering(
        data = flowsom_som$map$codes, 
        method = "metaClustering_consensus", 
        max = 20
      )
    metacluster_labels <- metaclusters[flowsom_som$map$mapping[, 1]]
    clusters <- data.frame(.flowsom_metacluster = metacluster_labels)
  }

```

### benchmarking function

```{r flowsom_benchmarking_function}
benchmark_clustering <- 
  function(file_paths) { 
    microbenchmark(
      flowsom_tidytof(file_names = file_paths), 
      flowsom_base(file_names = file_paths),
      times = num_repeats, 
      unit = "s" # seconds
    ) %>% 
      as_tibble() %>% 
      transmute(
        engine = if_else(str_detect(expr, "tidytof"), "tidytof", "base"), 
        time = time / (10^9)
      )
  }

file_name <- 
  in_out_paths %>% 
  filter(process == "flowsom") %>% 
  pull(file_name)

alternate_file_name <- 
  in_out_paths %>% 
  filter(process == "flowsom") %>% 
  pull(alternate_name)

```


```{r benchmark_flowsom, eval = run_cluster}
### perform benchmarking
cluster_benchmarking <- 
  ddpr_datasets_mini %>%
  mutate(cluster_benchmarks = map(.x = file_paths, .f = benchmark_clustering))
```

```{r save_flowsom_result, eval = run_cluster}
cluster_benchmarking %>% 
  write_rds(file = here::here("manuscript", "benchmarking", file_name))
```


```{r visualize_flowsom_result, message = FALSE}
if (!file.exists(here::here("manuscript", "benchmarking", file_name))) { 
  file_name <- alternate_file_name
}

cluster_benchmarking <-
  read_rds(file = here::here("manuscript", "benchmarking", file_name))

max_cells <- max(cluster_benchmarking$num_cells)
width <- max_cells * 0.03
  

engines <- 
  cluster_benchmarking %>% 
  select(cluster_benchmarks) %>% 
  unnest(cluster_benchmarks) %>% 
  pull(engine) %>% 
  unique()

num_engines <- length(engines)

my_palette <- charizard[which(names(charizard) %in% engines)]

flowsom_speed_plot <- 
  cluster_benchmarking %>% 
  select(num_files, num_cells, cluster_benchmarks) %>% 
  unnest(cols = cluster_benchmarks) %>% 
  group_by(num_files, num_cells, engine) %>% 
  summarize(
    mean_time = median(time), 
    max_time = quantile(time, 0.75), 
    min_time = quantile(time, 0.25)
  ) %>% 
  ungroup() %>% 
  ggplot(
    aes(
      x = num_cells, 
      y = mean_time, 
      fill = engine, 
      color = engine, 
      ymin = min_time, 
      ymax = max_time
    )
  ) + 
  geom_errorbar(width = width) + 
  geom_line() +
  geom_point(shape = 21, size = 2, color = "black") + 
  scale_x_continuous(labels = scales::label_scientific()) + 
  scale_fill_manual(values = my_palette) + 
  scale_color_manual(values = my_palette) +
  theme_bw() + 
  labs(
    subtitle = "FlowSOM speed benchmarking",
    x = "Number of cells", 
    y = "Time (seconds)", 
    fill = NULL, 
    color = NULL
  )

flowsom_speed_plot

flowsom_speed_plot %>% 
  ggsave(
    filename = 
      here::here(
        "manuscript", 
        "benchmarking", 
        str_replace(file_name, ".rds", ".pdf")
      ), 
    device = "pdf", 
    width = 5, 
    height = 3
  )

```

## Feature extraction

```{r feature_extraction_functions, eval = FALSE}
### {tidytof} function
extract_tidytof <- 
  function(data_frame) { 
    data_frame %>% 
      tof_extract_features(
        cluster_col = my_cluster, 
        group_cols = file_name,
        lineage_cols = starts_with("CD"), 
        signaling_cols = starts_with("p")
      )
  }

### Base R function
extract_base <- 
  function(data_frame) {
    lineage_cols <- grepl(pattern = "^CD", x = colnames(data_frame))
    signaling_cols <- grepl(pattern = "^p", x = colnames(data_frame))
    lineage_df <- data_frame[, lineage_cols]
    signaling_df <- data_frame[, signaling_cols]
    grouped_lineage_df <- 
      split(
        x = lineage_df, 
        f = list(data_frame$file_name, data_frame$my_cluster), 
        sep = "@"
      )
    grouped_signaling_df <- 
      split(
        x = signaling_df, 
        f = list(data_frame$file_name, data_frame$my_cluster), 
        sep = "@"
      )
    lineage_medians <- 
      sapply(
        X = grouped_lineage_df, 
        FUN = function(x) apply(X = x, MARGIN = 2, FUN = median)
      )
    signaling_thresh <- 
      sapply(
        X = grouped_signaling_df, 
        FUN = 
          function(x) {
            apply(X = x, MARGIN = 2, FUN = function(y) mean(y > asinh(10/5)))
          }
      )
    lineage_result <- as.data.frame(t(lineage_medians))
    signaling_result <- as.data.frame(t(signaling_thresh))
    lineage_result$my_cluster <- 
      substr(
        x = row.names(lineage_result), 
        start = nchar(row.names(lineage_result)), 
        stop = nchar(row.names(lineage_result))
      )
    lineage_result$file_name <- 
      sub(
        pattern = "@.$", 
        replacement = "", 
        x = row.names(lineage_result)
      )
    signaling_result$my_cluster <- 
      substr(
        x = row.names(signaling_result), 
        start = nchar(row.names(signaling_result)), 
        stop = nchar(row.names(signaling_result))
      )
    signaling_result$file_name <- 
      sub(
        pattern = "@.$", 
        replacement = "", 
        x = row.names(signaling_result)
      )
    num_files <- length(unique(signaling_result$file_name))
    num_clusters <- length(unique(signaling_result$my_cluster))
    num_signaling_channels <- 
      length(setdiff(colnames(signaling_result), c("file_name", "my_cluster")))
    num_lineage_channels <- 
      length(setdiff(colnames(lineage_result), c("file_name", "my_cluster")))
    
    lineage_final <- 
      matrix(
        data = 0, 
        nrow = num_files, 
        ncol = num_clusters * num_lineage_channels
      )
    row.names(lineage_final) <- unique(lineage_result$file_name)
    colname_grid <- 
      expand.grid(
        setdiff(colnames(lineage_result), c("file_name", "my_cluster")), 
        unique(lineage_result$my_cluster)
      )
    colnames(lineage_final) <- 
      paste(colname_grid$Var1, colname_grid$Var2, sep = "@")
    for (i in 1:nrow(lineage_final)) { 
      for (j in 1:ncol(lineage_final)) {
        file = row.names(lineage_final)[[i]]
        colname <- colnames(lineage_final)[[j]]
        my_cluster <- 
          substr(x = colname, start = nchar(colname), stop = nchar(colname))
        channel <- sub(pattern = "@.$", replacement = "", x = colname)
        current_value <- 
          lineage_result[
            lineage_result$my_cluster == my_cluster & lineage_result$file_nam == file, 
            channel
          ]
        lineage_final[i, j] <- current_value
      }
    }
    signaling_final <- 
      matrix(
        data = 0, 
        nrow = num_files, 
        ncol = num_clusters * num_signaling_channels
      )
    row.names(signaling_final) <- unique(lineage_result$file_name)
    colname_grid <- 
      expand.grid(
        setdiff(colnames(signaling_result), c("file_name", "my_cluster")), 
        unique(signaling_result$my_cluster)
      )
    colnames(signaling_final) <- 
      paste(colname_grid$Var1, colname_grid$Var2, sep = "@")
    for (i in 1:nrow(signaling_final)) { 
      for (j in 1:ncol(signaling_final)) {
        file = row.names(signaling_final)[[i]]
        colname <- colnames(signaling_final)[[j]]
        my_cluster <- 
          substr(x = colname, start = nchar(colname), stop = nchar(colname))
        channel <- sub(pattern = "@.$", replacement = "", x = colname)
        current_value <- 
          signaling_result[
            signaling_result$my_cluster == my_cluster & signaling_result$file_nam == file, 
            channel
          ]
        signaling_final[i, j] <- current_value
      }
    }
    result <- as.data.frame(cbind(lineage_final, signaling_final))
    return(result)
  }
```

### benchmarking function

```{r extraction_benchmarking_function}
benchmark_extract <- 
  function(data_frame) { 
    microbenchmark(
      extract_tidytof(data_frame = data_frame), 
      extract_base(data_frame = data_frame),
      times = num_repeats, 
      unit = "s" # seconds
    ) %>% 
      as_tibble() %>% 
      transmute(
        engine = if_else(str_detect(expr, "tidytof"), "tidytof", "base"), 
        time = time / (10^9)
      )
  }

file_name <- 
  in_out_paths %>% 
  filter(process == "extraction") %>% 
  pull(file_name)

alternate_file_name <- 
  in_out_paths %>% 
  filter(process == "extraction") %>% 
  pull(alternate_name)
```

### perform benchmarking

```{r benchmark_extraction, eval = run_extract}
extract_benchmarking <- 
  ddpr_data_mini %>%
  mutate(
    tibbles = 
      map(
        .x = tibbles, 
        .f = ~
          mutate(
            .x, 
            my_cluster = 
              sample(c("a", "b", "c", "d"), size = nrow(.x), replace = TRUE)
            )
        )
    ) %>% 
  mutate(extract_benchmarks = map(.x = tibbles, .f = benchmark_extract)) %>% 
  select(-tibbles, -flowSets)

```

```{r save_extraction_result, eval = run_extract}
# speed
extract_benchmarking %>% 
  write_rds(file = here::here("manuscript", "benchmarking", file_name))
```



```{r visualize_extraction_result, message = FALSE}
if (!file.exists(here::here("manuscript", "benchmarking", file_name))) { 
  file_name <- alternate_file_name
}

extract_benchmarking <- 
  read_rds(file = here::here("manuscript", "benchmarking", file_name))

max_cells <- max(extract_benchmarking$num_cells)
width <- max_cells * 0.03
  

engines <- 
  extract_benchmarking %>% 
  select(extract_benchmarks) %>% 
  unnest(extract_benchmarks) %>% 
  pull(engine) %>% 
  unique()

num_engines <- length(engines)

my_palette <- charizard[which(names(charizard) %in% engines)]

extract_speed_plot <- 
  extract_benchmarking %>% 
  select(num_files, num_cells, extract_benchmarks) %>% 
  unnest(cols = extract_benchmarks) %>% 
  group_by(num_files, num_cells, engine) %>% 
  summarize(
    mean_time = median(time), 
    max_time = quantile(time, 0.75), 
    min_time = quantile(time, 0.25)
  ) %>% 
  ungroup() %>% 
  ggplot(
    aes(
      x = num_cells, 
      y = mean_time, 
      fill = engine, 
      color = engine, 
      ymin = min_time, 
      ymax = max_time
    )
  ) + 
  geom_errorbar(width = width) +
  geom_line(aes(group = engine)) +
  geom_point(shape = 21, size = 2, color = "black") + 
  scale_x_continuous(labels = scales::label_scientific()) + 
  scale_fill_manual(values = my_palette) + 
  scale_color_manual(values = my_palette) + 
  theme_bw() + 
  labs(
    subtitle = "Feature extraction speed benchmarking",
    x = "Number of cells", 
    y = "Time (seconds)", 
    fill = NULL, 
    color = NULL
  )

extract_speed_plot

extract_speed_plot %>% 
  ggsave(
    filename = 
      here::here(
        "manuscript", 
        "benchmarking", 
        str_replace(file_name, ".rds", ".pdf")
      ), 
    device = "pdf", 
    width = 5, 
    height = 3
  )

```

## Lines of code

```{r analyze_code, message = FALSE, warning = FALSE}
prefixes <- 
  c("read_data", "preprocess", "downsample", "pca", "tsne", "umap", "flowsom", "extract")

analyses <- 
  c("reading files", "preprocessing", "downsampling", "pca", "tsne", "umap", "clustering", "feature extraction")

base_functions <- 
  mget(x = paste0(prefixes, "_base"))

tidytof_functions <- 
  mget(x = paste0(prefixes, "_tidytof"))

flowcore_functions <- 
  mget(x = paste0(prefixes, "_flowcore"), ifnotfound = NA)

get_lines <- function(function_object) { 
  if (is.na(function_object)) {
    return(NA)
  } else {
    lines <- capture.output(print(function_object))
    return(length(lines) - 2L)
  }
}

get_assignments <- function(function_object) {
  if (is.na(function_object)) {
    return(NA)
  } else {
    lines <- capture.output(print(function_object))
    assignments <- max(sum(str_count(lines, pattern = "<-")), 1L)
    return(assignments)
  }
}

# make tibble
code_tibble <- 
  tibble(
    analysis = factor(analyses, levels = analyses), 
    base_lines = map_int(.x = base_functions, .f = get_lines), 
    tidytof_lines = map_int(.x = tidytof_functions, .f = get_lines),
    flowcore_lines = map_int(.x = flowcore_functions, .f = get_lines),
    base_variables = map_int(.x = base_functions, .f = get_assignments), 
    tidytof_variables = map_int(.x = tidytof_functions, .f = get_assignments), 
    flowcore_variables = map_int(.x = flowcore_functions, .f = get_assignments)
  ) %>% 
  pivot_longer(
    cols = -analysis, 
    names_to = c("engine", "measure"), 
    values_to = "number", 
    names_sep = "_"
  )

code_tibble
```

```{r visualize_lines_of_code}
# visualize lines of code
engines <- 
  code_tibble %>% 
  pull(engine) %>% 
  unique()

my_palette <- charizard[names(charizard) %in% engines]

lines_plot_raw <- 
  code_tibble %>% 
  filter(measure == "lines") %>% 
  ggplot(aes(x = analysis, y = number, fill = engine)) + 
  # geom_line(
  #   mapping = aes(group = analysis),
  #   #data = filter(plot_tibble, engine == "tidytof"), 
  #   size = 1.5, 
  #   color = "gray60", 
  #   arrow = arrow(length = unit(0.075, "inches"), type = "closed"), 
  #   alpha = 0.9, 
  #   data = filter(code_tibble, measure == "lines") %>% 
  #     mutate(number = if_else(engine == "base", as.double(number), number + 4.5))
  # ) +
  geom_line(aes(group = engine, color = engine)) + 
  geom_point(shape = 21, size = 3) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
  scale_x_discrete(
    labels = 
      function(x) if_else(x %in% c("pca", "tsne", "umap"), str_to_upper(x), str_to_title(x))
  ) + 
  scale_fill_manual(values = my_palette) + 
  scale_color_manual(values = my_palette) + 
  labs(
    x = NULL, 
    y = "Lines of code", 
    fill = NULL, 
    color = NULL
  )

lines_plot_raw

```


```{r save_raw_lines}
lines_plot_raw %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", "lines_plot_raw.pdf"), 
    device = "pdf", 
    width = 5, 
    height = 3.5
  )
```


```{r analyze_relative_lines_base}
# tibble with reduction column (percent reduction in lines of code in tidytof
# compared to whatever engine it's being compared to)
plot_tibble_base <- 
  code_tibble %>% 
  filter(engine != "flowcore") %>% 
  filter(measure == "lines") %>% 
  group_by(analysis) %>% 
  mutate(
    lines = number, 
    number = number / max(number), 
    reduction = paste0(" (-", scales::label_percent()(1 - number), ")"), 
  ) %>% 
  ungroup()

lines_plot_base <- 
  plot_tibble_base %>% 
  ggplot(aes(x = analysis, y = number, fill = engine)) + 
  geom_segment(
    mapping = aes(x = analysis, y = 1, xend = analysis, yend = number + 0.04),
    data = filter(plot_tibble_base, engine == "tidytof"), 
    size = 1.5, 
    color = "gray60", 
    arrow = arrow(length = unit(0.1, "inches"), type = "closed"), 
    alpha = 0.9
  ) + 
  geom_hline(yintercept = 1, size = 1, color = "black", linetype = "dotted") + 
  geom_point(shape = 21, size = 4) + 
  geom_text(
    mapping = aes(label = reduction), 
    data = filter(plot_tibble_base, engine == "tidytof"), 
    nudge_x = 0, 
    nudge_y = -0.12, 
    size = 3
  ) + 
  geom_text(
    aes(color = engine, label = paste0(lines, " lines")), 
    data = filter(plot_tibble_base, engine == "base"), 
    nudge_y = 0.06, 
    show.legend = FALSE, 
    size = 3
  ) + 
  geom_text(
    aes(color = engine, label = paste0(lines, if_else(lines == 1, " line", " lines"))), 
    data = filter(plot_tibble_base, engine == "tidytof"), 
    nudge_y = -0.06, 
    show.legend = FALSE, 
    size = 3
  ) + 
  scale_x_discrete(
    labels = 
      function(x) if_else(x %in% c("pca", "tsne"), str_to_upper(x), str_to_title(x))
  ) + 
  scale_y_continuous(labels = scales::label_percent()) + 
  scale_fill_manual(values = my_palette) + 
  scale_color_manual(values = my_palette) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3)
  ) + 
  labs(
    x = NULL, 
    y = "Lines of code (% of base R)", 
    fill = NULL, 
    color = NULL
  )

lines_plot_base


```

```{r analyze_relative_lines_flowcore}
# tibble with reduction column (percent reduction in lines of code in tidytof
# compared to whatever engine it's being compared to)
plot_tibble_flowcore <- 
  code_tibble %>% 
  filter(engine != "base") %>% 
  filter(measure == "lines") %>% 
  filter(!(analysis %in% c("clustering", "feature extraction"))) %>% 
  group_by(analysis) %>% 
  mutate(
    lines = number, 
    number = number / max(number), 
    reduction = paste0(" (-", scales::label_percent()(1 - number), ")"), 
  ) %>% 
  ungroup()

lines_plot_flowcore <- 
  plot_tibble_flowcore %>% 
  ggplot(aes(x = analysis, y = number, fill = engine)) + 
  geom_segment(
    mapping = aes(x = analysis, y = 1, xend = analysis, yend = number + 0.04),
    data = filter(plot_tibble_flowcore, engine == "tidytof"), 
    size = 1.5, 
    color = "gray60", 
    arrow = arrow(length = unit(0.1, "inches"), type = "closed"), 
    alpha = 0.9
  ) + 
  geom_hline(yintercept = 1, size = 1, color = "black", linetype = "dotted") + 
  geom_point(shape = 21, size = 4) + 
  geom_text(
    mapping = aes(label = reduction), 
    data = filter(plot_tibble_flowcore, engine == "tidytof"), 
    nudge_x = 0, 
    nudge_y = -0.12, 
    size = 3
  ) + 
  geom_text(
    aes(color = engine, label = paste0(lines, " lines")), 
    data = filter(plot_tibble_flowcore, engine == "flowcore"), 
    nudge_y = 0.06, 
    show.legend = FALSE, 
    size = 3
  ) + 
  geom_text(
    aes(color = engine, label = paste0(lines, if_else(lines == 1, " line", " lines"))), 
    data = filter(plot_tibble_flowcore, engine == "tidytof"), 
    nudge_y = -0.06, 
    show.legend = FALSE, 
    size = 3
  ) + 
  scale_x_discrete(
    labels = 
      function(x) if_else(x %in% c("pca", "tsne", "umap"), str_to_upper(x), str_to_title(x))
  ) + 
  scale_y_continuous(labels = scales::label_percent()) + 
  scale_fill_manual(values = my_palette) + 
  scale_color_manual(values = my_palette) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3)
  ) + 
  labs(
    x = NULL, 
    y = "Lines of code (% of flowCore workflow)", 
    fill = NULL, 
    color = NULL
  )

lines_plot_flowcore


```




```{r save_relative_lines_plots}
lines_plot_base %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", "lines_plot_base.pdf"), 
    device = "pdf", 
    width = 6, 
    height = 4
  )

lines_plot_flowcore %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", "lines_plot_flowcore.pdf"), 
    device = "pdf", 
    width = 6, 
    height = 4
  )
```

```{r analyze_raw_variables, message = FALSE, warning = FALSE}
assignment_plot_raw <- 
  code_tibble %>% 
  filter(measure == "variables") %>% 
  ggplot(aes(x = analysis, y = number, fill = engine)) + 
  # geom_line(
  #   mapping = aes(group = analysis),
  #   #data = filter(plot_tibble, engine == "tidytof"), 
  #   size = 1, 
  #   color = "gray60", 
  #   arrow = arrow(length = unit(0.075, "inches"), type = "closed"), 
  #   alpha = 0.9, 
  #   data = filter(code_tibble, measure == "variables") %>% 
  #     mutate(number = if_else(engine == "base", as.double(number), number + 1.5))
  # ) +
  geom_line(aes(group = engine, color = engine)) + 
  geom_point(shape = 21, size = 3) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
  scale_x_discrete(
    labels = function(x) if_else(x %in% c("pca", "tsne", "umap"), str_to_upper(x), str_to_title(x))
  ) + 
  scale_fill_manual(values = my_palette) + 
  scale_color_manual(values = my_palette) + 
  labs(
    x = NULL, 
    y = "# of assigned variables", 
    fill = NULL, 
    color = NULL
  )

assignment_plot_raw

```

```{r save_raw_variables}
assignment_plot_raw %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", "assignment_plot_raw.pdf"), 
    device = "pdf", 
    width = 4.5, 
    height = 3.5
  )
```



```{r analyze_relative_variables_base}
# tibble with reduction column (percent reduction in variable assignments in tidytof
# compared to whatever engine it's being compared to)
plot_tibble_base <- 
  code_tibble %>% 
  filter(engine != "flowcore") %>% 
  filter(measure == "variables") %>% 
  group_by(analysis) %>% 
  mutate(
    lines = number, 
    number = number / max(number), 
    reduction = paste0(" (-", scales::label_percent()(1 - number), ")"), 
  ) %>% 
  ungroup()

variables_plot_base <- 
  plot_tibble_base %>% 
  ggplot(aes(x = analysis, y = number, fill = engine)) + 
  geom_segment(
    mapping = aes(x = analysis, y = 1, xend = analysis, yend = number + 0.04),
    data = filter(plot_tibble_base, engine == "tidytof"), 
    size = 1.5, 
    color = "gray60", 
    arrow = arrow(length = unit(0.1, "inches"), type = "closed"), 
    alpha = 0.9
  ) + 
  geom_hline(yintercept = 1, size = 1, color = "black", linetype = "dotted") + 
  geom_point(shape = 21, size = 4) + 
  geom_text(
    mapping = aes(label = reduction), 
    data = filter(plot_tibble_base, engine == "tidytof"), 
    nudge_x = 0, 
    nudge_y = -0.12, 
    size = 3
  ) + 
  geom_text(
    aes(color = engine, label = paste0(lines, " variables")), 
    data = filter(plot_tibble_base, engine == "base"), 
    nudge_y = 0.06, 
    show.legend = FALSE, 
    size = 3
  ) + 
  geom_text(
    aes(color = engine, label = paste0(lines, if_else(lines == 1, " variable", " variables"))), 
    data = filter(plot_tibble_base, engine == "tidytof"), 
    nudge_y = -0.06, 
    show.legend = FALSE, 
    size = 3
  ) + 
  scale_x_discrete(
    labels = 
      function(x) if_else(x %in% c("pca", "tsne", "umap"), str_to_upper(x), str_to_title(x))
  ) + 
  scale_y_continuous(labels = scales::label_percent()) + 
  scale_fill_manual(values = my_palette) + 
  scale_color_manual(values = my_palette) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3)
  ) + 
  labs(
    x = NULL, 
    y = "# of assigned variables (% of base R)", 
    fill = NULL, 
    color = NULL
  )

variables_plot_base


```

```{r analyze_relative_variables_flowcore}
# tibble with reduction column (percent reduction in variable assignments in tidytof
# compared to whatever engine it's being compared to)
plot_tibble_flowcore <- 
  code_tibble %>% 
  filter(engine != "base") %>% 
  filter(measure == "variables") %>% 
  filter(!(analysis %in% c("clustering", "feature extraction"))) %>% 
  group_by(analysis) %>% 
  mutate(
    lines = number, 
    number = number / max(number), 
    reduction = paste0(" (-", scales::label_percent()(1 - number), ")"), 
  ) %>% 
  ungroup()

variables_plot_flowcore <- 
  plot_tibble_flowcore %>% 
  ggplot(aes(x = analysis, y = number, fill = engine)) + 
  geom_segment(
    mapping = aes(x = analysis, y = 1, xend = analysis, yend = number + 0.04),
    data = filter(plot_tibble_flowcore, engine == "tidytof"), 
    size = 1.5, 
    color = "gray60", 
    arrow = arrow(length = unit(0.1, "inches"), type = "closed"), 
    alpha = 0.9
  ) + 
  geom_hline(yintercept = 1, size = 1, color = "black", linetype = "dotted") + 
  geom_point(shape = 21, size = 4) + 
  geom_text(
    mapping = aes(label = reduction), 
    data = filter(plot_tibble_flowcore, engine == "tidytof"), 
    nudge_x = 0, 
    nudge_y = -0.12, 
    size = 3
  ) + 
  geom_text(
    aes(color = engine, label = paste0(lines, " variables")), 
    data = filter(plot_tibble_flowcore, engine == "flowcore"), 
    nudge_y = 0.06, 
    show.legend = FALSE, 
    size = 3
  ) + 
  geom_text(
    aes(color = engine, label = paste0(lines, if_else(lines == 1, " variable", " variable"))), 
    data = filter(plot_tibble_flowcore, engine == "tidytof"), 
    nudge_y = -0.06, 
    show.legend = FALSE, 
    size = 3
  ) + 
  scale_x_discrete(
    labels = 
      function(x) if_else(x %in% c("pca", "tsne", "umap"), str_to_upper(x), str_to_title(x))
  ) + 
  scale_y_continuous(labels = scales::label_percent()) + 
  scale_fill_manual(values = my_palette) + 
  scale_color_manual(values = my_palette) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3)
  ) + 
  labs(
    x = NULL, 
    y = "# of assigned variables (% of flowCore workflow)", 
    fill = NULL, 
    color = NULL
  )

variables_plot_flowcore

```




```{r save_relative_variables_plots}
variables_plot_base %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", "variables_plot_base.pdf"), 
    device = "pdf", 
    width = 8, 
    height = 5
  )

variables_plot_flowcore %>% 
  ggsave(
    filename = 
      here::here("manuscript", "benchmarking", "variables_plot_flowcore.pdf"), 
    device = "pdf", 
    width = 6.5, 
    height = 5
  )
```









