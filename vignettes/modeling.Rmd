---
title: "Building predictive models"
author: "Timothy Keyes"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
description: > 
  Read this vignette to learn how to compute summary statistics like cluster 
  abundance and cluster marker expression using {tidytof} 
vignette: >
  %\VignetteIndexEntry{Modeling}
  %\VignetteEngine{knitr::knitr}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(tidytof)
library(dplyr)
library(ggplot2)
library(stringr)
```


`{tidytof}` implements several functions for building predictive models using sample- or patient-level data. 

## Accessing the data for this vignette

To illustrate how they work, first we download some patient-level data from [this paper](https://pubmed.ncbi.nlm.nih.gov/29505032/) and combining it with sample-level clinical annotations in one of `{tidytof}`'s built-in data objects (`ddpr_metadata`).


```{r}
# link for downloading the sample-level data from the Nature Medicine website
data_link <- 
  "https://static-content.springer.com/esm/art%3A10.1038%2Fnm.4505/MediaObjects/41591_2018_BFnm4505_MOESM3_ESM.csv"

# downloading the data and combining it with clinical annotations
ddpr_patients <- 
  readr::read_csv(data_link, skip = 2L, n_max = 78L, show_col_types = FALSE) %>% 
  dplyr::rename(patient_id = Patient_ID) %>% 
  left_join(ddpr_metadata, by = "patient_id") %>% 
  dplyr::filter(!str_detect(patient_id, "Healthy"))

ddpr_patients %>% 
  select(where(~ !is.numeric(.x))) %>% 
  head()
```

The data processing steps above result in the `ddpr_patients` tibble. The numeric columns in `ddpr_patients` represent aggregated cell population features for each sample (see [Supplementary Table 5 in this paper](https://www.nature.com/articles/nm.4505#MOESM3) for details). The non-numeric columns represent clinical metadata about each sample (run `?ddpr_metadata` for more information).

There are also a few preprocessing steps that we might want to perform now to save us some headaches when we're fitting models later. 

```{r}
ddpr_patients <- 
  ddpr_patients %>% 
  # convert the relapse_status variable to a factor first, 
  # which is something we'll want for fitting the model later
  # and create the time_to_event and event columns for survival modeling
  mutate(
    relapse_status = as.factor(relapse_status), 
    time_to_event = if_else(relapse_status == "Yes", time_to_relapse, ccr),
    event = if_else(relapse_status == "Yes", 1, 0)
  )
```

In the original DDPR paper, some patients were used to fit the model and the rest were used to assess the model after it was tuned. We can separate our training and validation cohorts using the `cohort` variable in `ddpr_patients`

```{r}
ddpr_training <- 
  ddpr_patients %>% 
  dplyr::filter(cohort == "Training") 

ddpr_validation <- 
  ddpr_patients %>% 
  dplyr::filter(cohort == "Validation")
```


```{r}
nrow(ddpr_training)

nrow(ddpr_validation)
```


## Building a classifier using elastic net-regularized logistic regression

First, we can build an elastic net classifier to predict which patients will relapse and which patients won't (ignoring time-to-event data for now). For this, we can use the `relapse_status` column in `ddpr_training` as the outcome variable: 

```{r}
# find how many of each outcome we have in our cohort
ddpr_training %>% 
  dplyr::count(relapse_status)
```

Specifically, we can use the `tof_split_data()` function to split our cohort into a training and test set either once (a "simple" split) or multiple times (using either k-fold cross-validation or bootstrapping). In this case, we use 5-fold cross-validation, but reading the documentation of `tof_split_data()` demonstrates how to use other methods. 

```{r}
training_split <- 
  ddpr_training %>% 
  tof_split_data(
    split_method = "k-fold", 
    num_cv_folds = 5, 
    strata = relapse_status
  )

training_split
```

The output of `tof_split_data()` varies depending on which `split_method` is used. For cross-validation, the result is a `rset` object from the [rsample](https://rsample.tidymodels.org/) package. `rset` objects are a type of tibble with two columns: 

* `splits` - a column in which each entry is an `rsplit` object (which contains a single resample of the full dataset)
* `id` - a character column in which each entry represents the name of the fold that each entry in `splits` belongs to.

We can inspect one of the resamples in the `splits` column to see what they contain:

```{r}
my_resample <- training_split$splits[[1]]

print(my_resample)

class(my_resample)
```

Note that you can use `rsample::training` and `rsample::testing` to return the training and test obeservations from each resampling: 

```{r}
my_resample %>% 
  rsample::training() %>% 
  head()

my_resample %>% 
  rsample::testing() %>% 
  head()
```

From here, we can feed  `training_split` into the `tof_train_model` function to [tune](https://www.tmwr.org/tuning.html) a logistic regression model that predicts the relapse_status of a leukemia patient. Be sure to check out the `tof_create_grid` documentation to learn how to make a hyperparameter search grid for model tuning (in this case, we limit the mixture parameter to a value of 1, which fits a sparse lasso model).

```{r, warning = FALSE}
class_mod <- 
  training_split %>% 
  tof_train_model(
    predictor_cols = contains("Pop2"), 
    response_col = relapse_status,
    model_type = "two-class", 
    hyperparameter_grid = tof_create_grid(mixture_values = 1), 
    impute_missing_predictors = TRUE, 
    remove_zv_predictors = TRUE # often a smart decision
  )
```

The output of `tof_train_model` is a `tof_model`, an object containing information about the trained model (and that can be passed to the `tof_predict` and `tof_assess_model` verbs). When a `tof_model` is printed, some information about the optimal hyperparamters is printed, and so is a table of the nonzero model coefficients in the model. 

```{r}
print(class_mod)
```


We can then use the trained model to make predictions on the validation data that we set aside earlier: 

```{r}
class_predictions <- 
  class_mod %>% 
  tof_predict(new_data = ddpr_validation, prediction_type = "class")

ddpr_validation %>% 
  dplyr::select(relapse_status) %>% 
  bind_cols(class_predictions) %>% 
  dplyr::count(relapse_status, .pred)

accuracy <- 
  ddpr_validation %>% 
  dplyr::select(relapse_status) %>% 
  bind_cols(class_predictions) %>% 
  mutate(.pred = factor(.pred, levels = levels(relapse_status))) %>% 
  yardstick::accuracy(truth = relapse_status, estimate = .pred) %>% 
  pull(.estimate) %>% 
  round(3)
```

So we can see that our accuracy is about `r accuracy`.

We can also assess the model directly using `tof_assess_model`

```{r}
# calling the function with no new_data gives us the assessment on 
# the training data
training_assessment <- 
  class_mod %>% 
  tof_assess_model()

training_assessment
```

And we can make an ROC curve using our metrics: 

```{r}
auc <- 
  training_assessment$model_metrics %>% 
  dplyr::filter(metric == "roc_auc") %>% 
  dplyr::pull(value)

training_assessment$roc_curve %>% 
  ggplot(aes(x = FPR, y = TPR)) + 
  geom_line() + 
  geom_abline(
    slope = 1, 
    intercept = 0, 
    linetype = "dotted", 
    color = "gray60"
  ) + 
  labs(
    subtitle = "Training performance", 
    caption = str_glue("AUC = {auc}", auc = auc)
  ) + 
  theme_bw()

```

We can then assess the model on the validation data 

```{r}
validation_assessment <- 
  class_mod %>% 
  tof_assess_model(new_data = ddpr_validation)

validation_assessment
```


```{r}
auc <- 
  validation_assessment$model_metrics %>% 
  dplyr::filter(metric == "roc_auc") %>% 
  dplyr::pull(value)

validation_assessment$roc_curve %>% 
  ggplot(aes(x = FPR, y = TPR)) + 
  geom_line() + 
  geom_abline(
    slope = 1, 
    intercept = 0, 
    linetype = "dotted", 
    color = "gray60"
  ) + 
  labs(
    subtitle = "Validation performance", 
    caption = str_glue("AUC = {auc}", auc = round(auc, 3))
  ) + 
  theme_bw()

```


## Building a survival model using elastic net-regularized cox regression

```{r, warning = FALSE}
survival_mod <- 
  training_split %>% 
  tof_train_model(
    predictor_cols = contains("Pop2"), 
    time_col = time_to_event, 
    event_col = event, 
    model_type = "survival", 
    hyperparameter_grid = tof_create_grid(mixture_values = 1), 
    impute_missing_predictors = TRUE, 
    remove_zv_predictors = TRUE # often a smart decision
  )

print(survival_mod)
```

Making predictions using the survival model

```{r}
survival_mod %>% 
  tof_predict(new_data = ddpr_validation, prediction_type = "response")
```

Assessing the survival model 

```{r}
survival_assessment <- 
  survival_mod %>% 
  tof_assess_model(new_data = ddpr_validation)

survival_assessment
```

And we can use some of these values to make a survival curve for each patient 

```{r, message = FALSE}
survival_assessment$survival_curves %>% 
  tidyr::unnest(cols = survival_curve) %>% 
  left_join(
    y = 
      tibble(
        ddpr_risk = ddpr_validation$ddpr_risk, 
        row_index = as.character(1:nrow(ddpr_validation))
      ), 
    by = "row_index"
  ) %>% 
  # something is weird about the first patient
  dplyr::filter(row_index != "1") %>% 
  dplyr::group_by(time, ddpr_risk) %>% 
  dplyr::summarize(probability = mean(probability)) %>% 
  ggplot(aes(x = time, y = probability, color = ddpr_risk)) + 
  geom_path() +
  theme_bw() 
```

